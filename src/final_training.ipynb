{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d59ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def process_absa_dataset(input_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Converts an ABSA dataset from raw JSONL format to a flat format with text, aspect, and sentiment columns.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input .jsonl file\n",
    "        output_path (str): Path to the output .csv file\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line.strip())\n",
    "            text = entry.get(\"text\", \"\")\n",
    "            labels = entry.get(\"labels\", [])\n",
    "\n",
    "            for label in labels:\n",
    "                start, end, aspect_sentiment = label\n",
    "                if \"_\" in aspect_sentiment:\n",
    "                    *aspect_parts, sentiment = aspect_sentiment.split(\"_\")\n",
    "                    aspect = \"_\".join(aspect_parts)\n",
    "                else:\n",
    "                    aspect, sentiment = aspect_sentiment, \"neutral\"\n",
    "\n",
    "\n",
    "                processed_data.append({\n",
    "                    \"text\": text,\n",
    "                    \"aspect\": aspect,\n",
    "                    \"sentiment\": sentiment\n",
    "                })\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Processed dataset saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b23d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file = \"review_with_aspect.jsonl\"\n",
    "output_file = \"aspect_based_sentiment.csv\"\n",
    "process_absa_dataset(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b927a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your preprocessed dataset\n",
    "df = pd.read_csv(\"aspect_based_sentiment.csv\")\n",
    "\n",
    "# Optional: label encoding\n",
    "label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "df[\"label\"] = df[\"sentiment\"].map(label_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Encode text + aspect as sentence pairs\n",
    "def encode_pair(row):\n",
    "    return tokenizer(\n",
    "        row[\"text\"],\n",
    "        row[\"aspect\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Example encoding (batching will come later)\n",
    "sample = df.iloc[0]\n",
    "encoded = encode_pair(sample)\n",
    "\n",
    "print(encoded.keys())  # Shows input_ids, attention_mask, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1040eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            row[\"text\"],\n",
    "            row[\"aspect\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(row[\"label\"])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Model\n",
    "class ModernBertClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=3, dropout=0.3, classifier_layers=1, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        in_features = self.bert.config.hidden_size\n",
    "\n",
    "        if(classifier_layers == 1):\n",
    "            self.classifier = nn.Linear(in_features, num_labels)\n",
    "        elif classifier_layers == 2:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(in_features, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, num_labels)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"classifier_layers must be 1 or 2\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = output.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(self.dropout(cls_output))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f762629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "\n",
    "train_dataset = ABSADataset(train_df, tokenizer)\n",
    "val_dataset = ABSADataset(val_df, tokenizer)\n",
    "test_dataset = ABSADataset(test_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train size: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate(model, dataloader, compute_loss=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            if compute_loss:\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    avg_loss = total_loss / len(dataloader) if compute_loss else None\n",
    "    return acc, f1, avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a083b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline 1 - Majority class\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# Create and train dummy classifier\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit([[0]] * len(y_train), y_train)  # Fake features, labels only matter\n",
    "\n",
    "# Predict and evaluate\n",
    "dummy_preds = dummy.predict([[0]] * len(y_test))\n",
    "acc_dummy = accuracy_score(y_test, dummy_preds)\n",
    "f1_dummy = f1_score(y_test, dummy_preds, average=\"macro\")\n",
    "\n",
    "print(f\"Majority Class Baseline - Accuracy: {acc_dummy:.4f} | F1 Score: {f1_dummy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ffeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline 2 - Logictic regression with TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Combine text + aspect for TF-IDF baseline (mimicking ABSA input)\n",
    "train_texts = (train_df[\"text\"] + \" [ASPECT] \" + train_df[\"aspect\"]).tolist()\n",
    "test_texts = (test_df[\"text\"] + \" [ASPECT] \" + test_df[\"aspect\"]).tolist()\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "baseline_preds = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, baseline_preds)\n",
    "f1 = f1_score(y_test, baseline_preds, average=\"macro\")\n",
    "\n",
    "print(f\"Logistic Regression Baseline - Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "#Login wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sweep config: Random search for LR, Dropout, and Architecture\n",
    "sweep_config = {\n",
    "    'method': 'random',  # grid search is tooo expensive.\n",
    "    'metric': {\n",
    "      'name': 'val_f1',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [1e-5, 2e-5, 3e-5]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.1, 0.3, 0.5]\n",
    "        },\n",
    "        'classifier_layers': {\n",
    "            'values': [1, 2]   # 1 for single linear, 2 for hidden+output\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128] #256 does not fit in memory\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ef221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm  # <--- import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config, name=\"runs-with-5-epoch\"):\n",
    "        config = wandb.config\n",
    "\n",
    "        # ----- Model -----\n",
    "        model = ModernBertClassifier(\n",
    "            model_name=\"answerdotai/ModernBERT-base\",\n",
    "            num_labels=3,\n",
    "            dropout=config.dropout,\n",
    "            classifier_layers=config.classifier_layers\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "        \n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # ----- Data Loaders -----\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=8\n",
    "        )\n",
    "\n",
    "        # ----- Optimizer -----\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        num_epochs = 5  # Keep it short due to complexity. \n",
    "        #Previous experiments showed that 5 epochs is enough for convergence.\n",
    "\n",
    "        best_macro_f1 = 0.0\n",
    "        for epoch in range(num_epochs):\n",
    "            # ----- TRAINING -----\n",
    "            model.train()\n",
    "            train_losses, train_labels, train_preds = [], [], []\n",
    "            train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "            for batch in train_pbar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                train_labels.extend(labels.cpu().numpy())\n",
    "                train_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            train_acc = accuracy_score(train_labels, train_preds)\n",
    "            train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "\n",
    "            # ----- VALIDATION -----\n",
    "            model.eval()\n",
    "            val_losses, val_labels, val_preds = [], [], []\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "            with torch.no_grad():\n",
    "                for batch in val_pbar:\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    val_losses.append(loss.item())\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    val_labels.extend(labels.cpu().numpy())\n",
    "                    val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            val_acc = accuracy_score(val_labels, val_preds)\n",
    "            val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "            \n",
    "            # Print summary for your monitoring\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                  f\"Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | \"\n",
    "                  f\"Train Loss: {sum(train_losses)/len(train_losses):.4f} | \"\n",
    "                  f\"Val Loss: {sum(val_losses)/len(val_losses):.4f}\")\n",
    "\n",
    "            # Log to WANDB\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": sum(train_losses) / len(train_losses),\n",
    "                \"train_acc\": train_acc,\n",
    "                \"train_f1\": train_f1,\n",
    "                \"val_loss\": sum(val_losses) / len(val_losses),\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_f1\": val_f1\n",
    "            })\n",
    "\n",
    "            # Save best\n",
    "            if val_f1 > best_macro_f1:\n",
    "                best_macro_f1 = val_f1\n",
    "\n",
    "        # Log best macro-F1 at the end\n",
    "        wandb.log({\"best_val_macro_f1\": best_macro_f1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"aspect-sentiment-modernbert\")\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "\n",
    "wandb.agent(sweep_id, function=train_sweep, count=5)  # Change count as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30831a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_config = {\n",
    "    \"learning_rate\": 3e-5,        \n",
    "    \"dropout\": 0.3,              \n",
    "    \"classifier_layers\": 1,       \n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 3             \n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_model = ModernBertClassifier(\n",
    "    model_name=\"answerdotai/ModernBERT-base\",\n",
    "    num_labels=3,\n",
    "    dropout=best_config[\"dropout\"],\n",
    "    classifier_layers=best_config[\"classifier_layers\"]\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs via DataParallel!\")\n",
    "    best_model = torch.nn.DataParallel(best_model)\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "# ---- Prepare DataLoaders ----\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_config[\"batch_size\"], shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_config[\"batch_size\"], shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_config[\"batch_size\"], shuffle=False, num_workers=8)\n",
    "\n",
    "# ---- Training Loop ----\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(best_model.parameters(), lr=best_config[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_f1 = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(best_config[\"num_epochs\"]):\n",
    "    best_model.train()\n",
    "    train_labels, train_preds = [], []\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    print(f\"Epoch {epoch+1} Train F1: {train_f1:.4f} Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    best_model.eval()\n",
    "    val_labels, val_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1} Val F1: {val_f1:.4f} Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model_state = best_model.state_dict()  # Save best model\n",
    "\n",
    "# ---- Load the best model weights ----\n",
    "if best_model_state is not None:\n",
    "    best_model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ea127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Evaluation on Test Set ----\n",
    "best_model.eval()\n",
    "test_labels, test_preds = [], []\n",
    "test_loss_values = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = best_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_loss_values.append(loss.item())\n",
    "\n",
    "test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_loss = sum(test_loss_values) / len(test_loss_values)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"aspect-sentiment-modernbert\",\n",
    "    name=\"final_test_eval\" \n",
    ")\n",
    "\n",
    "wandb.log({\n",
    "    \"test_accuracy\": test_acc,\n",
    "    \"test_macro_f1\": test_f1,\n",
    "    \"test_loss\": test_loss\n",
    "})\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test F1 Score: {test_f1:.4f} | Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d6081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pre_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7fadcc9901d0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7faf26f1b4d0, raw_cell=\"#lime\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "class_name..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B34.77.197.199/home/ubuntu/ERDEM/nlp/new/new.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenPipeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:603\u001b[39m, in \u001b[36m_WandbInit._pre_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backend.interface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    602\u001b[39m     \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mpausing backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:770\u001b[39m, in \u001b[36mInterfaceBase.publish_pause\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    769\u001b[39m     pause = pb.PauseRequest()\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:289\u001b[39m, in \u001b[36mInterfaceShared._publish_pause\u001b[39m\u001b[34m(self, pause)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb.PauseRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(pause=pause)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, local)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[33m\"\u001b[39m\u001b[33mpb.Record\u001b[39m\u001b[33m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mself\u001b[39m._assign(record)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[39m, in \u001b[36mSockClient.send_record_publish\u001b[39m\u001b[34m(self, record)\u001b[39m\n\u001b[32m    172\u001b[39m server_req.request_id = record.control.mailbox_slot\n\u001b[32m    173\u001b[39m server_req.record_publish.CopyFrom(record)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[39m, in \u001b[36mSockClient.send_server_request\u001b[39m\u001b[34m(self, msg)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[39m, in \u001b[36mSockClient._send_message\u001b[39m\u001b[34m(self, msg)\u001b[39m\n\u001b[32m    149\u001b[39m header = struct.pack(\u001b[33m\"\u001b[39m\u001b[33m<BI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m), raw_size)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[39m, in \u001b[36mSockClient._sendall_with_error_handle\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    128\u001b[39m start_time = time.monotonic()\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     sent = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sent == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mBrokenPipeError\u001b[39m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 18180 - True: positive\n",
      "Predicted class: negative\n",
      "Sample 11495 - True: positive\n",
      "Predicted class: positive\n",
      "Sample 11571 - True: positive\n",
      "Predicted class: positive\n",
      "Sample 7069 - True: negative\n",
      "Predicted class: negative\n",
      "Sample 13251 - True: neutral\n",
      "Predicted class: neutral\n",
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7fadcc9901d0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7faf26f1a490, execution_count=46 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7faf26f1b4d0, raw_cell=\"#lime\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "class_name..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B34.77.197.199/home/ubuntu/ERDEM/nlp/new/new.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenPipeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:614\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    613\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:778\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    777\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:293\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    292\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, local)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[33m\"\u001b[39m\u001b[33mpb.Record\u001b[39m\u001b[33m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mself\u001b[39m._assign(record)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[39m, in \u001b[36mSockClient.send_record_publish\u001b[39m\u001b[34m(self, record)\u001b[39m\n\u001b[32m    172\u001b[39m server_req.request_id = record.control.mailbox_slot\n\u001b[32m    173\u001b[39m server_req.record_publish.CopyFrom(record)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[39m, in \u001b[36mSockClient.send_server_request\u001b[39m\u001b[34m(self, msg)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[39m, in \u001b[36mSockClient._send_message\u001b[39m\u001b[34m(self, msg)\u001b[39m\n\u001b[32m    149\u001b[39m header = struct.pack(\u001b[33m\"\u001b[39m\u001b[33m<BI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m), raw_size)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[39m, in \u001b[36mSockClient._sendall_with_error_handle\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    128\u001b[39m start_time = time.monotonic()\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     sent = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sent == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mBrokenPipeError\u001b[39m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "#lime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "if isinstance(best_model, torch.nn.DataParallel):\n",
    "    best_model = best_model.module\n",
    "best_model.cpu()  # Now REALLY on CPU\n",
    "\n",
    "\n",
    "def lime_predict(texts):\n",
    "    \"\"\"\n",
    "    texts: list of \"aspect [SEP] text\" strings\n",
    "    \"\"\"\n",
    "    aspects = []\n",
    "    sentences = []\n",
    "    for s in texts:\n",
    "        # Split on [SEP] for aspect and text\n",
    "        if \"[SEP]\" in s:\n",
    "            parts = s.split(\"[SEP]\")\n",
    "            aspect = parts[0].strip()\n",
    "            text = parts[1].strip()\n",
    "        else:\n",
    "            # fallback if [SEP] missing\n",
    "            aspect = \"\"\n",
    "            text = s.strip()\n",
    "        aspects.append(aspect)\n",
    "        sentences.append(text)\n",
    "\n",
    "    # Tokenize as pairs\n",
    "    encoded = tokenizer(\n",
    "        sentences,\n",
    "        aspects,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoded['input_ids'].to('cpu')\n",
    "    attention_mask = encoded['attention_mask'].to('cpu')\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = best_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "num_examples = 5\n",
    "import random\n",
    "random.seed(333)\n",
    "indices = random.sample(range(len(test_df)), num_examples)\n",
    "\n",
    "random.seed(15)\n",
    "indices2 = random.sample(range(len(test_df)), num_examples)\n",
    "\n",
    "#pls note that for some of the examples in the report indices2 are used.\n",
    "for idx in indices2:  \n",
    "    row = test_df.iloc[idx]\n",
    "    input_text = f\"{row['aspect']} [SEP] {row['text']}\"\n",
    "    true_label = row['label']\n",
    "    print(f\"Sample {idx} - True: {class_names[true_label]}\")\n",
    "    exp = explainer.explain_instance(\n",
    "        input_text,\n",
    "        lime_predict,\n",
    "        num_features=10,\n",
    "        labels=[0, 1, 2],\n",
    "        num_samples=500\n",
    "    )\n",
    "    pred_class = np.argmax(lime_predict([input_text])[0])\n",
    "    print(\"Predicted class:\", class_names[pred_class])\n",
    "    #exp.show_in_notebook(text=input_text)\n",
    "    exp.save_to_file(f'lime_explanation_{idx}.html')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
